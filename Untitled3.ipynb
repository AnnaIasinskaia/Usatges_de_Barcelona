{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7a924c07335c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-7a924c07335c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mconfigFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m     \u001b[0mconfigString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigString\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"required parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import sys\n",
    "from collections import OrderedDict, Counter\n",
    "from numpy.random import choice\n",
    "import time\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import copy\n",
    "\n",
    "def runLDA(corpus, iterations, alpha, beta):\n",
    "    \"\"\"An implementation of Latent Dirichlet Allocation. Probabilistically\n",
    "        generates \"topics\" for a given corpus, each of which contains many\n",
    "        words that are related by their coocurrence in the text. Uses the\n",
    "        CorpusData data structure containing information about word location\n",
    "        and outputs a list of the words in each topic to the shell after the\n",
    "        desired number of iterations.\n",
    "\n",
    "    Args:\n",
    "        corpus (CorpusData): A data structure that has already called \"loadData\"\n",
    "            on a text.\n",
    "        iterations (int): The desired number of iterations for the LDA algorithm.\n",
    "            More iterations lead to more consistent, coherent topics at the cost of\n",
    "            a longer runtime.\n",
    "        alpha (float): The first \"hyperparameter\" or \"smoothing constant.\" Affects\n",
    "            the P(w|t) calculation. When alpha is higher, documents tend to\n",
    "            represent a greater variety of topics.\n",
    "        beta (float): Another hyperparameter, this one affecting the P(t|d)\n",
    "            calculation. A higher value for beta causes topics to contain a greater\n",
    "            variety of words.\n",
    "\n",
    "    \"\"\"\n",
    "    printProgressBar(0, iterations, prefix='Progress', suffix='complete', length=50)\n",
    "    for i in range(0, iterations):\n",
    "        # getting start time to measure runtime\n",
    "        # delete the line below for the final release!\n",
    "        startTime = time.clock()\n",
    "        for doc in range(len(corpus.wordLocationArray)):\n",
    "            for word in range(len(corpus.wordLocationArray[doc])):\n",
    "                oldTopic = corpus.topicAssignmentByLoc[doc][word]\n",
    "                corpus.removeWordFromDataStructures(word, doc, oldTopic)\n",
    "                #import pdb; pdb.set_trace()\n",
    "                wordProbabilities = corpus.calculateProbabilities(doc, word, alpha, beta)\n",
    "                newTopic = choice(range(len(wordProbabilities)), p=wordProbabilities)\n",
    "                corpus.addWordToDataStructures(word, doc, newTopic)\n",
    "        estTime = math.ceil((time.clock() - startTime) * (iterations - i) / 60)\n",
    "        time.sleep(0.1)\n",
    "        if i == iterations-1:\n",
    "            printProgressBar(i + 1, iterations, prefix='Progress', suffix='complete', length=50)\n",
    "        elif (estTime > 0):\n",
    "            printProgressBar(i + 1, iterations, prefix='Progress', suffix='complete', length=50, estTimeRemaining=estTime)\n",
    "        else:\n",
    "            printProgressBar(i + 1, iterations, prefix='Progress', suffix='complete', length=50)\n",
    "\n",
    "\n",
    "# class that stores words from a text and organizes them in various ways to facilitate LDA\n",
    "# organization can be by location, by document, by word, and topic\n",
    "# the methods load in text, encode data, and output topics in various ways\n",
    "class CorpusData:\n",
    "    # Location Information\n",
    "    # 2d array: outer array contains documents which are arrays of words in the order they appear\n",
    "    wordLocationArray = []\n",
    "    # 2d array: outer array contains documents which are arrays of topics which exactly match the words\n",
    "    # in the previous array\n",
    "    topicAssignmentByLoc = []\n",
    "\n",
    "    #data structures used for creating the annotated text\n",
    "    #wordLocArrayStatic is wordLocationArray with stopwords included\n",
    "    #topicAssignByLocStatic is topicAssignmentByLoc with stopwords included\n",
    "    wordLocArrayStatic = []\n",
    "    topicAssignByLocStatic = []\n",
    "\n",
    "    # Word Information\n",
    "    # dictionary mapping unique words to the number of times they appear\n",
    "    uniqueWordDict = {}\n",
    "    # dictionary mapping unique words to an array indexed by topic of how many times they\n",
    "    # appear in each topic\n",
    "    wordDistributionAcrossTopics = {}\n",
    "\n",
    "    # Topic Information\n",
    "    # array of topics, where topics are dictionaries\n",
    "    # keys are words and the values are counts for that word\n",
    "    topicWordInstancesDict = []\n",
    "    # array of numbers, where each number is the number of words in the topic (corresponding by index)\n",
    "    topicTotalWordCount = []\n",
    "\n",
    "    # Document Information\n",
    "    # array of documents, each document is an array\n",
    "    # each index is a topic\n",
    "    # each value is number of words in the document that belong to that topic\n",
    "    docTopicalWordDist = []\n",
    "    # a list of of the number of words in each document\n",
    "    docTotalWordCounts = []\n",
    "    #a list of punctuation\n",
    "    punctuation = []\n",
    "    #the locations of the punctuation\n",
    "    puncLocations = []\n",
    "\n",
    "    # consideration: the way the csv is organized could vary. should we standardize it as\n",
    "    # pat of preprocessing? we are currently using the format given by wikiParse.py\n",
    "    # working csv\n",
    "    file = \"\"\n",
    "\n",
    "    # number of topics we want in the output\n",
    "\n",
    "    numTopics = 0\n",
    "\n",
    "    stopwords = []\n",
    "\n",
    "    def __init__(self, file, numTopics):\n",
    "        \"\"\"Constructor for CorpusData.\n",
    "\n",
    "        Args:\n",
    "            file (str): The path to a csv file storing the words in a corpus and their documents.\n",
    "            numTopics (int): The number of topics to be output by this instance of LDA.\n",
    "\n",
    "        Returns:\n",
    "            CorpusData: An instance of the corpus data class that only contains a file and number of topics.\n",
    "\n",
    "        \"\"\"\n",
    "        self.file = file\n",
    "        self.numTopics = numTopics\n",
    "\n",
    "    # reads the csv and loads the appropriate data structures. may be refactored by struct\n",
    "    # stopLowerBound and stopUpperBound are floats between 0 and 1\n",
    "    # they represent what proportion of documents a word can appear in without getting filtered out\n",
    "    # stopWhitelist and stopBlacklist are two lists of strings\n",
    "    # strings in stopWhitelist will not be filtered out even if they are outside the document bounds\n",
    "    # strings in stopBlacklist will be filtered out even if they are inside the document bounds\n",
    "    def loadData(self, stopLowerBound, stopUpperBound, stopWhitelist, stopBlacklist):\n",
    "        \"\"\"Reads the csv and loads the data structures used in LDA.\n",
    "\n",
    "        Args:\n",
    "            stopLowerBound (float): The minimum percentage of documents a word must appear in\n",
    "                to be included in the algorithm.\n",
    "            stopUpperBound (float): The maximum percentage of documents a word can appear in\n",
    "                to be included in the algorithm.\n",
    "            stopWhitelist (list): A list of words that should never be filtered out of the algorithm.\n",
    "            stopBlacklist (list): A list of words that should always be filtered out of the algorithm.\n",
    "\n",
    "        \"\"\"\n",
    "        with open(self.file, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            wordsColumn = []\n",
    "            docColumn = []\n",
    "            curDoc = \"\"\n",
    "            curDocIndex = -1\n",
    "            # load our 2d wordsByLocation array\n",
    "            for row in reader:\n",
    "                # add the word to the current array if word's doc is curDoc\n",
    "                if curDoc == row[1]:\n",
    "                    self.wordLocationArray[curDocIndex].append(row[0].lower())\n",
    "                # add the word to a new doc array if word's doc is not curDoc\n",
    "                else:\n",
    "                    curDoc = row[1]\n",
    "                    curDocIndex += 1\n",
    "                    self.wordLocationArray.append([])\n",
    "                    self.wordLocationArray[curDocIndex].append(row[0].lower())\n",
    "                # have a list representing each column in the doc\n",
    "                wordsColumn.append(row[0].lower())\n",
    "                docColumn.append(row[1])\n",
    "\n",
    "                # load word counts into dictionary\n",
    "        self.uniqueWordDict = Counter(wordsColumn)\n",
    "        self.wordLocArrayStatic = copy.deepcopy(self.wordLocationArray)\n",
    "\n",
    "        # removes stopwords\n",
    "        wordDocCounts = dict.fromkeys(self.uniqueWordDict, 0)\n",
    "        for doc in self.wordLocationArray:\n",
    "            wordInDoc = []\n",
    "            for word in doc:\n",
    "                if word not in wordInDoc:\n",
    "                    wordInDoc.append(word)\n",
    "                    wordDocCounts[word] += 1\n",
    "        if stopLowerBound == \"off\":\n",
    "            stopLowerBound = 0\n",
    "        if stopUpperBound == \"off\":\n",
    "            stopUpperBound = 2\n",
    "\n",
    "        lowerBound = math.ceil(len(self.wordLocationArray) * float(stopLowerBound))\n",
    "        upperBound = math.ceil(len(self.wordLocationArray) * float(stopUpperBound))\n",
    "\n",
    "        # create an array of stopwords\n",
    "        for word in wordDocCounts:\n",
    "            if wordDocCounts[word] <= lowerBound or wordDocCounts[word] >= upperBound:\n",
    "                self.stopwords.append(word)\n",
    "        for bannedWord in stopBlacklist:\n",
    "            if bannedWord not in self.stopwords:\n",
    "                self.stopwords.append(bannedWord)\n",
    "        for allowedWord in stopWhitelist:\n",
    "            if allowedWord in self.stopwords:\n",
    "                self.stopwords.remove(allowedWord)\n",
    "\n",
    "        # remove all stopwords from wordLocationArray and uniqueWordDict\n",
    "        self.stopwords = set(self.stopwords)\n",
    "\n",
    "        for docWords in self.wordLocationArray:\n",
    "            docWords[:] = [w for w in docWords if w not in self.stopwords]\n",
    "        for w in self.stopwords:\n",
    "            self.uniqueWordDict.pop(w, None)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        # count words in each document (docWordCounts)\n",
    "        docSet = list(OrderedDict.fromkeys(docColumn))\n",
    "        for doc in docSet:\n",
    "            self.docTotalWordCounts.append(docColumn.count(doc))\n",
    "\n",
    "        # build topicsByLocation by going through each topic in a loop\n",
    "        for i in range(len(self.wordLocationArray)):\n",
    "            self.topicAssignmentByLoc.append([0] * len(self.wordLocationArray[i]))\n",
    "        chosenTopic = 0\n",
    "        for i in range(len(self.wordLocationArray)):\n",
    "            for j in range(len(self.wordLocationArray[i])):\n",
    "                chosenTopic += 1\n",
    "                chosenTopic %= self.numTopics\n",
    "                # randTopic = random.randrange(self.numTopics)\n",
    "                # self.topicsByLocation[i][j] = randTopic\n",
    "                self.topicAssignmentByLoc[i][j] = chosenTopic\n",
    "\n",
    "        # create wordTopicCounts using the information in topicsByLocation\n",
    "        for i in range(len(self.wordLocationArray)):\n",
    "            for j in range(len(self.wordLocationArray[i])):\n",
    "                word = self.wordLocationArray[i][j]\n",
    "                assignedTopic = self.topicAssignmentByLoc[i][j]\n",
    "                if word not in self.wordDistributionAcrossTopics:\n",
    "                    self.wordDistributionAcrossTopics[word] = [0] * self.numTopics\n",
    "                self.wordDistributionAcrossTopics[word][assignedTopic] += 1\n",
    "\n",
    "        # create docList\n",
    "        for i in range(len(self.wordLocationArray)):\n",
    "            self.docTopicalWordDist.append([])\n",
    "            for j in range(self.numTopics):\n",
    "                self.docTopicalWordDist[i].append(0)\n",
    "        # i indexes by document in topicsbylocation, then the topic number becomes an index\n",
    "        for i in range(len(self.topicAssignmentByLoc)):\n",
    "            for topic in self.topicAssignmentByLoc[i]:\n",
    "                self.docTopicalWordDist[i][topic] += 1\n",
    "\n",
    "        # loading topicList and topicWordCounts\n",
    "        self.topicTotalWordCount = [0] * self.numTopics\n",
    "        self.topicWordInstancesDict = [{} for _ in range(self.numTopics)]\n",
    "        for word in self.wordDistributionAcrossTopics:\n",
    "            wordTopics = self.wordDistributionAcrossTopics[word]\n",
    "            for i in range(self.numTopics):\n",
    "                self.topicWordInstancesDict[i][word] = wordTopics[i]\n",
    "                self.topicTotalWordCount[i] += wordTopics[i]\n",
    "                # if wordTopics[i] != 0:\n",
    "                #    self.topicWordCounts[i] += 1\n",
    "\n",
    "    def printTopics(self):\n",
    "        \"\"\"Prints each topic in topicList on a new line in the format \"Topic 1: word1, word2,\n",
    "            word3, ...\" The words are sorted from highest to lowest incidence in the topic.\n",
    "\n",
    "        \"\"\"\n",
    "        for topic in self.topicWordInstancesDict:\n",
    "            print(\"Topic \" + str(self.topicWordInstancesDict.index(topic) + 1) + \": \"),\n",
    "            print(\", \".join(sorted(topic, key=topic.get, reverse=True)))\n",
    "\n",
    "    def encodeData(self, readfile, topics, iterations, alpha, beta, outputname, puncData):\n",
    "        \"\"\"Encodes information about the LDA output in a .json file. Stores the\n",
    "            name of the input file, number of topics, number of iterations,\n",
    "            hyperparameters, and data structures used to build the topics.\n",
    "\n",
    "        Args:\n",
    "            readfile (str): Name of the file given as input to LDA (with file extension).\n",
    "            topics (int): Number of topics generated by this run of LDA.\n",
    "            iterations (int): Number of iterations of this run of LDA.\n",
    "            alpha (float): Alpha constant used in this run of LDA.\n",
    "            beta (float): Beta constant used in this run of LDA.\n",
    "            outputname (str): Name of the desired output JSON file (without file extension).\n",
    "            puncData ([[str]]): Catalogue of tokens in the file that include punctuation or capitalization\n",
    "\n",
    "        \"\"\"\n",
    "        for doc in self.topicAssignByLocStatic:\n",
    "            for location in range(len(doc)):\n",
    "                doc[location] = int(doc[location])\n",
    "        for i in range(len(self.topicWordInstancesDict)):\n",
    "            self.topicWordInstancesDict[i] = {k:v for k,v in self.topicWordInstancesDict[i].items() if v}\n",
    "\n",
    "        dumpDict = {'dataset': readfile[:-4],\n",
    "                    'topics': topics,\n",
    "                    'iterations': iterations,\n",
    "                    'alpha': alpha,\n",
    "                    'beta': beta,\n",
    "                    'wordsByLocationWithStopwords': self.wordLocArrayStatic,\n",
    "                    'topicsByLocationWithStopwords': self.topicAssignByLocStatic,\n",
    "                    'topicWordInstancesDict': self.topicWordInstancesDict,\n",
    "                    'stopwords': list(self.stopwords),\n",
    "                    'puncAndCap': puncData[0],\n",
    "                    'puncCapLocations': puncData[1],\n",
    "                    'newlineLocations': puncData[2]}\n",
    "                    # 'puncAndCap': puncData[0], <--potential restructure\n",
    "                    # 'newlineLocations': puncData[1]} <--potential restructure\n",
    "        outputfile = outputname+\".json\"\n",
    "        with open(outputfile, 'w') as outfile:\n",
    "            json.dump(dumpDict, outfile, indent=4)\n",
    "\n",
    "    def removeWordFromDataStructures(self, word, doc, oldTopic):\n",
    "        \"\"\"Removes an instance of a word from a topic and updates the\n",
    "            appropriate data structures accordingly.\n",
    "\n",
    "        Args:\n",
    "            word (int): The index of the given word in its home document.\n",
    "            doc (int): The index of the given document in wordLocationArray.\n",
    "            oldTopic (int): The index of the topic from which the word in\n",
    "                question is being removed.\n",
    "\n",
    "        \"\"\"\n",
    "        wordString = self.wordLocationArray[doc][word]\n",
    "        self.topicWordInstancesDict[oldTopic][wordString] -= 1\n",
    "        self.topicTotalWordCount[oldTopic] -= 1\n",
    "        self.docTopicalWordDist[doc][oldTopic] -= 1\n",
    "\n",
    "    def addWordToDataStructures(self, word, doc, newTopic):\n",
    "        \"\"\"Adds an instance of a word to a topic and updates the approrpriate\n",
    "            data structures accordingly.\n",
    "\n",
    "        Args:\n",
    "            word (int): The index of the given word in its home document.\n",
    "            doc (int): The index of the given document in wordLocationArray.\n",
    "            newTopic (int): The index of the topic to which the word in\n",
    "                question is being added.\n",
    "        \"\"\"\n",
    "        wordString = self.wordLocationArray[doc][word]\n",
    "        self.topicAssignmentByLoc[doc][word] = newTopic\n",
    "        self.topicWordInstancesDict[newTopic][wordString] += 1\n",
    "        self.topicTotalWordCount[newTopic] += 1\n",
    "        self.docTopicalWordDist[doc][newTopic] += 1\n",
    "\n",
    "    def calculateProbabilities(self, docCoord, wordCoord, alpha, beta):\n",
    "        \"\"\"Given an instance of a word and two smoothing constants, returns\n",
    "            a list of probabilities that a word will be assigned to each topic.\n",
    "\n",
    "        Args:\n",
    "            docCoord (int): The index of the document in question.\n",
    "            wordCoord (int): The index of the desired word in that document.\n",
    "            alpha (float): A constant default value for the P(w|t) calculation.\n",
    "            beta (float): A constant default value for the P(t|d) calculation.\n",
    "\n",
    "        Returns:\n",
    "            [float]: A list of normalized probabilities that the given word\n",
    "            will appear in each topic. Each index in this list corresponds to a\n",
    "            topic, and the probability associated with the topic is used in\n",
    "            runLDA to determine to which topic a word should be assigned.\n",
    "\n",
    "        \"\"\"\n",
    "        word = self.wordLocationArray[docCoord][wordCoord]\n",
    "        newWordProbs = []\n",
    "        for i in range(len(self.topicWordInstancesDict)):\n",
    "            # pwt = P(w|t)\n",
    "            topicDict = self.topicWordInstancesDict[i]\n",
    "            wordCount = topicDict[word]\n",
    "            pwt = (wordCount + beta) / (self.topicTotalWordCount[i] + beta)\n",
    "            # ptd = P(t|d)\n",
    "            wordsInTopicInDoc = self.docTopicalWordDist[docCoord][i]\n",
    "            #import pdb; pdb.set_trace()\n",
    "            ptd = (wordsInTopicInDoc + alpha) / (self.docTotalWordCounts[docCoord] + alpha)\n",
    "            # ptw = P(t|w)\n",
    "            ptw = pwt * ptd\n",
    "            newWordProbs.append(ptw)\n",
    "        # normalize probabilities\n",
    "        normalizedProbabilities = []\n",
    "        rawsum = sum(newWordProbs)\n",
    "        for probability in newWordProbs:\n",
    "            if rawsum == 0:\n",
    "                normalizedProbabilities.append(0.0)\n",
    "            else:\n",
    "                normalizedProbabilities.append(probability / rawsum)\n",
    "        return normalizedProbabilities\n",
    "\n",
    "    def outputAsCSV(self, outputname):\n",
    "        \"\"\"Creates a .csv file containing readable results from the LDA run.\n",
    "            Each topic has three columns: Word, Count (number of times the\n",
    "            word appears in that topic) and Percentage (percentage of that\n",
    "            topic that is the given word).\n",
    "\n",
    "        Args:\n",
    "            outputname (str): The desired name (with no file extension) of the\n",
    "                .csv output file.\n",
    "\n",
    "        \"\"\"\n",
    "        loadData = []\n",
    "        largestTopic = max(self.topicTotalWordCount)\n",
    "        for i in range(largestTopic + 2):\n",
    "            new = []\n",
    "            for j in range(self.numTopics * 3):\n",
    "                new.append(0)\n",
    "            loadData.append(new)\n",
    "\n",
    "        # formatting\n",
    "        for i in range(self.numTopics):\n",
    "            loadData[0][3 * i] = ''\n",
    "            loadData[0][3 * i + 1] = 'Topic' + str(i + 1)\n",
    "            loadData[0][3 * i + 2] = ''\n",
    "            loadData[1][3 * i] = 'Word'\n",
    "            loadData[1][3 * i + 1] = 'Count'\n",
    "            loadData[1][3 * i + 2] = 'Percentage'\n",
    "\n",
    "        for i in range(self.numTopics):\n",
    "            topicAsList = []\n",
    "            for k, v in self.topicWordInstancesDict[i].items():\n",
    "                percent = (v / self.topicTotalWordCount[i]) * 100\n",
    "                topicAsList.append([k, v, percent])\n",
    "            topicAsList.sort(key=itemgetter(1), reverse=True)\n",
    "            for j in range(len(topicAsList)):\n",
    "                loadData[j + 2][3 * i] = topicAsList[j][0]\n",
    "                loadData[j + 2][3 * i + 1] = topicAsList[j][1]\n",
    "                loadData[j + 2][3 * i + 2] = topicAsList[j][2]\n",
    "        outputfile = outputname + \".csv\"\n",
    "        with open(outputfile, 'w', newline='') as csvfile:\n",
    "            filewriter = csv.writer(csvfile, delimiter=',')\n",
    "            count = 0\n",
    "            for row in loadData:\n",
    "                if count < 200:\n",
    "                    filewriter.writerow(row)\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    def createAnnoTextDataStructure(self):\n",
    "        \"\"\"Creates \"static\" versions of several data structures such that they\n",
    "            contain stop words. Used in the creation of the annotated text.\n",
    "\n",
    "        \"\"\"\n",
    "        stopwordTopic = -1\n",
    "        for document in range(len(self.wordLocationArray)):\n",
    "            docTopicList = []\n",
    "            counter = 0\n",
    "            for word in range(len(self.wordLocArrayStatic[document])):\n",
    "                if len(self.wordLocationArray[document]) > counter:\n",
    "                    if self.wordLocArrayStatic[document][word] == self.wordLocationArray[document][counter]:\n",
    "                        docTopicList.append(self.topicAssignmentByLoc[document][counter])\n",
    "                        # self.topicAssignByLocStatic.append(int(self.topicAssignmentByLoc[document][counter])) <--potential restructure\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        docTopicList.append(stopwordTopic)\n",
    "                        #self.topicAssignByLocStatic(int(stopwordTopic)) <--potential restructure\n",
    "                else:\n",
    "                    docTopicList.append(stopwordTopic)\n",
    "            self.topicAssignByLocStatic.append(docTopicList)\n",
    "\n",
    "def grabPuncAndCap(fileName):\n",
    "    \"\"\"Given .txt file, iterates through to create data structures containing all words\n",
    "            that are attached to punctuation or contain capitalization. Also stores the\n",
    "            location of the new line characters. Purpose: for loading in a user-friendly\n",
    "            version of the text in the UI Annotated Text section.\n",
    "\n",
    "        Args:\n",
    "            fileName (str): The input file given by the user, if the input file is a .txt\n",
    "\n",
    "        Returns:\n",
    "            puncAndCap ([str]): A list of words, in the order they appear and as they appear\n",
    "            in the .txt, that are attached to punctuation or contain capitalization\n",
    "            puncCapLocations ([str]): A list of indexes that corresponds with the words in\n",
    "            puncAndCap. The indexes indicate where in the full text each word in puncAndCap\n",
    "            belongs.\n",
    "            newlineLocations ([str]): A list of indexes. Each index indicates where a new line\n",
    "            character belongs in the Annotated Text.\n",
    "\n",
    "        \"\"\"\n",
    "    fileString = open(fileName, 'r').read().split()\n",
    "    unsplitFile = open(fileName, 'r').read()\n",
    "    newlineLocations = []\n",
    "    count = 0\n",
    "    trackToken = ''\n",
    "    ##remove starting white space from file\n",
    "    removeStartWhitespace = False\n",
    "    while not removeStartWhitespace:\n",
    "        if unsplitFile[0] == ' ' or unsplitFile[0] == '\\t' or unsplitFile[0] == '\\n':\n",
    "            unsplitFile = unsplitFile[1:]\n",
    "        else:\n",
    "            removeStartWhitespace = True\n",
    "    ##get locations of new line characters\n",
    "    for i in range(len(unsplitFile)):\n",
    "        if unsplitFile[i] == \"\\n\":\n",
    "            if trackToken != '':\n",
    "                newlineLocations.append(count)\n",
    "                trackToken = ''\n",
    "                count += 1\n",
    "            else:\n",
    "                newlineLocations.append(newlineLocations[len(newlineLocations)-1])\n",
    "        elif unsplitFile[i] == '\\t' or unsplitFile[i] == ' ':\n",
    "            if unsplitFile[i+1] != '\\t' and unsplitFile[i+1] != ' ' and unsplitFile[i+1] != \"\\n\":\n",
    "                if trackToken != '':\n",
    "                    trackToken = ''\n",
    "                    count += 1\n",
    "        else:\n",
    "            trackToken += unsplitFile[i]\n",
    "\n",
    "    ##get punctuation and capitalization info\n",
    "    puncAndCap = []\n",
    "    puncCapLocations = []\n",
    "    count = 0\n",
    "    for token in fileString:\n",
    "        allPunc = False\n",
    "        if '.' in token or ',' in token or '!' in token or '?' in token or '\"' in token or '(' in token or ')' in token or ':' in token or ';' in token or '“' in token or '”' in token or '‘' in token or '’' in token or \"'\" in token or any(ltr for ltr in token if ltr.isupper()):\n",
    "            allPunc = True\n",
    "            for char in token:\n",
    "                if char != \".\" and char != \",\" and char != \"!\" and char != \"?\" and char != '\"' and char != \"(\" and char != \")\" and char != \":\" and char != ';' and char != '“' and char != '”' and char != '‘' and char != '’' and char != \"'\":\n",
    "                    allPunc = False\n",
    "            puncAndCap.append(token)\n",
    "            if allPunc:\n",
    "                puncCapLocations.append(count - 0.5)\n",
    "            else:\n",
    "                puncCapLocations.append(count)\n",
    "        if not allPunc:\n",
    "            count += 1\n",
    "    return puncAndCap, puncCapLocations, newlineLocations\n",
    "    #return fileString, newlineLocations <--potential restructure\n",
    "\n",
    "\n",
    "def txtToCsv(fileName, splitString):\n",
    "    \"\"\"Given a .txt file containing the corpus, creates a .csv file that chunks the\n",
    "        corpus based on splitString. The .csv file has two columns: the first contains\n",
    "        words in the order they appear in the corpus, and the second contains the\n",
    "        document (denoted by an integer starting from 1) that contains the word.\n",
    "\n",
    "    Args:\n",
    "        fileName (str): The name of the .txt file being read and converted.\n",
    "        splitString (str): The string generated by makeChunkString that\n",
    "            gives instructions on how to split up the txt file into documents.\n",
    "\n",
    "    \"\"\"\n",
    "    fileString = open(fileName, 'r').read().lower()\n",
    "    wordList = fileString.split()\n",
    "    if splitString[:3] == 'num':\n",
    "        numDocs = int(splitString[3:])\n",
    "        docLength = len(wordList) // numDocs\n",
    "        docStringsArray = getDocsOfLength(docLength, wordList, True)\n",
    "    #to have a fixed length document, input \"lengthXX\" for documents of length XX\n",
    "    elif splitString[:6] == 'length':\n",
    "        docLength = int(splitString[6:])\n",
    "        docStringsArray = getDocsOfLength(docLength, wordList, False)\n",
    "    else: \n",
    "        docStringsArray = fileString.split(splitString.lower())\n",
    "        temp = [docStringsArray[0]]\n",
    "        for i in range(1, len(docStringsArray)):\n",
    "            temp.append(splitString + docStringsArray[i])\n",
    "\n",
    "        docStringsArray = temp\n",
    "    print(\"Number of documents: \" + str(len(docStringsArray)))\n",
    "    for i in range(len(docStringsArray)):\n",
    "        docStringsArray[i] = docStringsArray[i].replace(\"\\n\", \" \")\n",
    "    csvfilename = fileName[:-4]+\".csv\"\n",
    "    with open(csvfilename, 'w', newline='') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',')\n",
    "        currentDoc = 1\n",
    "        for docString in docStringsArray:\n",
    "            wordsArray = docString.split(' ')\n",
    "            for word in wordsArray:\n",
    "                word = word.strip('.,!?\"“”‘’():;\\n\\t\\'')\n",
    "                if word != '':\n",
    "                    filewriter.writerow([word,str(currentDoc)])\n",
    "            currentDoc += 1\n",
    "\n",
    "def getDocsOfLength(docLen, wordList, numCap):\n",
    "    \"\"\"Divides a list of words up by documents with a desired length. If the words\n",
    "        don't divide evenly into this number, the last document will either be\n",
    "        longer or shorter than the rest depending on the circumstances.\n",
    "\n",
    "    Args:\n",
    "        docLen (int): The number of words that should appear in each document.\n",
    "        wordList ([str]): A list containing all of the words in a corpus split\n",
    "            by whitespace.\n",
    "        numCap (bool): True if a desired number of documents was specified and\n",
    "            an additional \"stub document\" would be a problem. False otherwise.\n",
    "\n",
    "    Returns:\n",
    "        docStringsArray ([[str]]): A list of documents, each containing (docLen)\n",
    "            words in the order they appear in the corpus. If numCap was True and\n",
    "            wordList doesn't divide evenly by docLen, the last element of this\n",
    "            list will be longer than the rest. Otherwise, whether there is a\n",
    "            longer final document or a shorter final document depends on whether\n",
    "            len(wordList) % docLen is less than or greater than docLen / 2\n",
    "            respectively.\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Length of each document: \" + str(docLen))\n",
    "    docStringsArray = []\n",
    "    while wordList:\n",
    "        doc = \" \".join(str(wordList[i]) for i in range(min(docLen, len(wordList))))\n",
    "        wordList = wordList[docLen:]\n",
    "        docStringsArray.append(doc)\n",
    "    lastDocLen = len(docStringsArray[-1].split())\n",
    "    #if we have a fixed number of documents, we want to stick a \"stub\" document onto the last one\n",
    "    #otherwise, we will do it if it is under half the desired document length\n",
    "    if (numCap and lastDocLen < docLen) or \\\n",
    "    (not numCap and lastDocLen < docLen // 2):\n",
    "        stubDoc = docStringsArray.pop()\n",
    "        appendString = \" \" + stubDoc\n",
    "        docStringsArray[-1] += appendString\n",
    "    return docStringsArray\n",
    "\n",
    "def makeChunkString(chunkType, chunkParam):\n",
    "    \"\"\"Creates a string that instructs txtToCsv how to split up documents.\n",
    "\n",
    "    Args:\n",
    "        chunkType (str): \"number of documents\" or \"length of documents\" depending on\n",
    "            whether documents should be into a given number or by a given length\n",
    "            (by number of words), or 'string' if a document should be split on a\n",
    "            given string.\n",
    "        chunkParam (int): Depending on chunkType, represents the desired number of\n",
    "            documents or the desired length of documents in number of words.\n",
    "\n",
    "    Returns:\n",
    "        str: A string such as \"num200\" \"length7150\" or \"\\n\\n\\n\" that instructs txtToCsv\n",
    "            how to split up a text file into documents.\n",
    "\n",
    "    \"\"\"\n",
    "    chunkString = ''\n",
    "    if chunkType == 'number of documents':\n",
    "        chunkString += 'num'\n",
    "        chunkString += str(chunkParam)\n",
    "    elif chunkType == 'length of documents':\n",
    "        chunkString += 'length'\n",
    "        chunkString += str(chunkParam)\n",
    "    elif chunkType == 'split string':\n",
    "        chunkString = chunkParam\n",
    "    elif chunkType == 'using csv':\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Invalid chunkType given.\\n\")\n",
    "        exit()\n",
    "    return chunkString\n",
    "\n",
    "# progressbar by user Greenstick on stackoverflow modified to include estimated time remaining\n",
    "def printProgressBar(iteration, total, prefix='', suffix='', decimals=1, length=100, fill='█', estTimeRemaining=0):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        estTimeRemaining - Optional : custom parameter added to original version of function\n",
    "        :param timeremaining:\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    if estTimeRemaining == 1:\n",
    "        sys.stdout.write('\\r%s |%s| %s%% %s %s %s' % (prefix, bar, percent, (suffix + '; Estimated time remaining: '), estTimeRemaining, 'minute'))\n",
    "    elif estTimeRemaining > 0:\n",
    "        sys.stdout.write('\\r%s |%s| %s%% %s %s %s' % (prefix, bar, percent, (suffix + '; Estimated time remaining: '), estTimeRemaining, 'minutes'))\n",
    "    else:\n",
    "        sys.stdout.write('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix))\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "def main():\n",
    "    \"\"\" Uses config.json to be run straight from the shell with no\n",
    "        arguments: \"python3 LDA.py [name of config file].json\". Calls virtually every other\n",
    "        function in the file. Topics are generated according to the settings\n",
    "        in config.json and relevant data is placed in [outputname].json.\n",
    "        Note: The config file must be in the same directory as LDA.py and must be\n",
    "        formatted properly.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python3 LDA.py [config file name].json\")\n",
    "        exit()\n",
    "    configFile = sys.argv[1]\n",
    "    configString = open(configFile, 'r').read()\n",
    "    config = json.loads(configString)\n",
    "    source = config[\"required parameters\"][\"source\"]\n",
    "    iterations = config[\"required parameters\"][\"iterations\"]\n",
    "    topics = config[\"required parameters\"][\"topics\"]\n",
    "    outputname = config[\"required parameters\"][\"output name\"]\n",
    "    upperlimit = config[\"stopword options\"][\"upper limit\"]\n",
    "    lowerlimit = config[\"stopword options\"][\"lower limit\"]\n",
    "    whitelist = config[\"stopword options\"][\"whitelist\"]\n",
    "    blacklist = config[\"stopword options\"][\"blacklist\"]\n",
    "    chunkingoptions = config[\"chunking options\"]\n",
    "    chunkType = \"\"\n",
    "    chunkParam = 0\n",
    "    for option in chunkingoptions.keys():\n",
    "        if chunkingoptions[option] != \"off\":\n",
    "            chunkType = option\n",
    "            chunkParam = chunkingoptions[option]\n",
    "            break\n",
    "    alpha = config[\"hyperparameters\"][\"alpha\"]\n",
    "    beta = config[\"hyperparameters\"][\"beta\"]\n",
    "    chunkString = makeChunkString(chunkType, chunkParam)\n",
    "    if source[-3:] == 'txt':\n",
    "        puncData = grabPuncAndCap(source)\n",
    "        txtToCsv(source, chunkString)\n",
    "        source = source[:-4] + \".csv\"\n",
    "    else:\n",
    "        puncData = [[], [], []]\n",
    "\n",
    "    corpus = CorpusData(source, topics)\n",
    "    corpus.loadData(lowerlimit, upperlimit, whitelist, blacklist)\n",
    "    runLDA(corpus, iterations, alpha, beta)\n",
    "    corpus.createAnnoTextDataStructure()\n",
    "    corpus.encodeData(source, topics, iterations, alpha, beta, outputname, puncData)\n",
    "\n",
    "    # clean up words from topics that have value 0 (i.e. are not assigned to that topic)\n",
    "    for topic in corpus.topicWordInstancesDict:\n",
    "        for key in list(topic.keys()):\n",
    "            if topic[key] == 0:\n",
    "                del topic[key]\n",
    "    corpus.outputAsCSV(outputname)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
